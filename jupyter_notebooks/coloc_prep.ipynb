{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Author: Peter Riesebos\n",
    "- Purpose: (outdated code) Script used to prepare the eQTL sum stat and GWAS sum stat data for colocalization\n",
    "- Input: eQTL and gwas summary statistics\n",
    "- Output: table to be used for colocalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library and file imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import json\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas = pd.read_csv(\"/groups/umcg-fg/tmp04/projects/gut-bulk/ongoing/2024-02-07-GutPublicRNASeq/datasets/GWAS/GCST90292538.h.tsv.gz\", sep='\\t')\n",
    "sum = pd.read_csv(\"/groups/umcg-fg/tmp04/projects/gut-bulk/ongoing/2024-02-07-GutPublicRNASeq/datasets/combined/mbqtl_output_combined_exp_fixed/merged_topeffects_final.txt\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Maak een lijst met genen met een significant eQTL\n",
    "2. Maak een lijst met varianten die in de eQTL studie getest zijn en die binnen 200kb van de tss liggen van genen uit stap 1\n",
    "3. filter de eQTL all effects files op de genen en varianten uit stap 1 en 2.\n",
    "4. Filter de GWAS sumstats op basis van de lijst varianten uit stap 2\n",
    "5. Voor elk significant gen: bepaal of binnen 200kb van de tss (TSS positie staat in de eqtl output file als GenePos oid) een gwas snp is met p<5e-8, die ook getest is in de eQTL analyse (e.g. moet voorkomen in de lijst van stap 2).\n",
    "6. Voor elk gen waar dit het geval is:\n",
    "- Pak alle varianten binnen 200kb van de tss (upstream en downstream)  uit de eQTL all effects sumstats\n",
    "- pak de overlappende varianten uit de GWAS sumstats\n",
    "- run coloc\n",
    "\n",
    "Sla de volgende waardes op per gen waarvoor je coloc hebt gerunned, maak een tabel:\n",
    "gennaam, gensymbol, PP3, PP4, top GWAS SNP + pvalue,  top eQTL SNP + p-value, afstand tussen beide snps, afstand van beide top snps tot de TSS van het gen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1: create a list of genes with a significant eQTL\n",
    "\n",
    "list1 = sum[[\"Gene\", \"GenePos\", \"SNP\"]]\n",
    "\n",
    "signif_eqtl_genes = sum['Gene'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(signif_eqtl_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # step 2: make a list with variants that are both tested in the eQTL study and are within 200kb from the tss from step 1\n",
    "\n",
    "# # Calculate the absolute distance between GenePos (TSS) and SNPPos\n",
    "# sum[\"distance_to_tss\"] = (sum[\"GenePos\"] - sum[\"SNPPos\"]).abs()\n",
    "\n",
    "# # Filter for SNPs within 200 kb (200,000 base pairs)\n",
    "# within_200kb = sum[sum[\"distance_to_tss\"] < 200_000]\n",
    "\n",
    "# # Extract the SNPs that meet the criteria into a list\n",
    "# snp_within_200kb = within_200kb[\"SNP\"].tolist()\n",
    "# print(len(snp_within_200kb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2 revised: filter the 22 eQTL AllEffects\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Define list1 of genes to filter on\n",
    "gene_set = set(list1['Gene'])  # Convert to a set for faster lookup\n",
    "\n",
    "# Paths for the 22 files\n",
    "file_paths = [f\"/groups/umcg-fg/tmp04/projects/gut-bulk/ongoing/2024-02-07-GutPublicRNASeq/datasets/combined/mbqtl_output_combined_exp_fixed/combined_chr{num}-AllEffects.txt.gz\" for num in range(1, 23)]\n",
    "\n",
    "# Function to filter a single file\n",
    "def filter_file(file_path):\n",
    "    file_name = file_path.split(\"/\")[-1]\n",
    "    logging.info(f\"Starting to process {file_name}\")\n",
    "    df = pd.read_csv(file_path, sep=\"\\t\", compression=\"gzip\")\n",
    "    filtered_df = df[df['Gene'].isin(gene_set)]\n",
    "    logging.info(f\"Finished processing {file_name} with {len(filtered_df)} matching rows\")\n",
    "    return filtered_df\n",
    "\n",
    "# Process files in parallel\n",
    "filtered_results = []\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    # Map the filter function to each file path\n",
    "    results = executor.map(filt er_file, file_paths)\n",
    "    # Collect results\n",
    "    filtered_results = list(results)\n",
    "\n",
    "# Optional: Combine results into a single DataFrame\n",
    "combined_filtered_df = pd.concat(filtered_results, ignore_index=True)\n",
    "combined_filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save / load in output_file from step 2:\n",
    "# output_file = \"AllEffectsFiltered.csv\"\n",
    "# combined_filtered_df.to_csv(output_file, index=False)\n",
    "combined_filtered_df = pd.read_csv(\"AllEffectsFiltered.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Create a dictionary to store results\n",
    "rsid_dict = {}\n",
    "\n",
    "# Define the window size in base pairs\n",
    "window_size = 200_000\n",
    "\n",
    "# Function to process each gene in parallel\n",
    "def process_gene(row):\n",
    "    gene = row['Gene']\n",
    "    gene_pos = row['GenePos']\n",
    "\n",
    "    # Log progress for each Gene, GenePos\n",
    "    logging.info(f\"Processing Gene: {gene}, GenePos: {gene_pos}\")\n",
    "\n",
    "    # Filter combined_filtered_df for the current Gene and 200kb window around GenePos\n",
    "    filtered_rows = combined_filtered_df[\n",
    "        (combined_filtered_df['Gene'] == gene) &\n",
    "        (combined_filtered_df['SNPPos'] >= gene_pos - window_size) &\n",
    "        (combined_filtered_df['SNPPos'] <= gene_pos + window_size)\n",
    "    ]\n",
    "\n",
    "    # Extract the rsids (SNP column) within this window and convert to a list\n",
    "    rsid_list = filtered_rows['SNP'].tolist()\n",
    "    \n",
    "    return (gene, gene_pos), rsid_list\n",
    "\n",
    "# Using ThreadPoolExecutor to run the tasks in parallel\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = {executor.submit(process_gene, row): row for index, row in list1.iterrows()}\n",
    "    \n",
    "    # Collect results as they are completed\n",
    "    for future in as_completed(futures):\n",
    "        (gene, gene_pos), rsid_list = future.result()\n",
    "        rsid_dict[(gene, gene_pos)] = rsid_list\n",
    "\n",
    "# Log final summary\n",
    "logging.info(f\"Completed processing {len(list1)} genes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert tuple keys to comma-separated strings\n",
    "# json_compatible_dict = {f\"{k[0]},{k[1]}\": v for k, v in rsid_dict.items()}\n",
    "\n",
    "# # Save to a JSON file\n",
    "# with open('gene_snp_rsid_mapping.json', 'w') as file:\n",
    "#     json.dump(json_compatible_dict, file)\n",
    "\n",
    "# Load from the JSON file and convert keys back to tuples\n",
    "with open('gene_snp_rsid_mapping.json', 'r') as file:\n",
    "    loaded_dict = json.load(file)\n",
    "    loaded_dict = {tuple(k.split(',')): v for k, v in loaded_dict.items()}\n",
    "    print(loaded_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(loaded_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all values into one list and remove duplicates\n",
    "all_values = list(set(value for values in loaded_dict.values() for value in values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3: filter the 22 eQTL AllEffects files on the genes and variants from step 1 and 2\n",
    "\n",
    "# Paths for the 22 files\n",
    "file_paths = [f\"/groups/umcg-fg/tmp04/projects/gut-bulk/ongoing/2024-02-07-GutPublicRNASeq/datasets/combined/mbqtl_output_combined_exp_fixed/combined_chr{num}-AllEffects.txt.gz\" for num in range(1, 23)]\n",
    "\n",
    "# Create a Dask DataFrame by loading all files at once\n",
    "df = dd.read_csv(file_paths, sep='\\t', compression='gzip')\n",
    "\n",
    "# Filter rows where the Gene is in signif_eqtl_genes and SNP is in snp_within_200kb\n",
    "filtered_df = df[(df['Gene'].isin(signif_eqtl_genes)) & (df['SNP'].isin(signif_eqtl_genes))]\n",
    "\n",
    "# Compute the filtered result to get a pandas DataFrame\n",
    "final_output = filtered_df.compute()\n",
    "\n",
    "# Display the result\n",
    "final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment when needed:\n",
    "# save step 3 data\n",
    "# final_output.to_csv(\"filtered_eqtl_results.tsv.gz\", sep='\\t', index=False, compression='gzip')\n",
    "\n",
    "# load in data to skip step 3\n",
    "final_output = pd.read_csv(\"filtered_eqtl_results.tsv.gz\", sep='\\t', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4: filter the GWAS sum stats on the list of step 2\n",
    "filtered_gwas = gwas[gwas['rsid'].isin(signif_eqtl_genes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_gwas.base_pair_location.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: \n",
    "significant_gwas_snps = filtered_gwas[filtered_gwas['p_value'] < 5e-8]\n",
    "\n",
    "# Initialize a list to store results\n",
    "results = []\n",
    "\n",
    "# Define the window size in base pairs\n",
    "window_size = 200000  # 200 kb\n",
    "\n",
    "# Iterate over each gene in `sum`\n",
    "for _, gene_row in sum.iterrows():\n",
    "    gene = gene_row['Gene']\n",
    "    gene_pos = gene_row['GenePos']\n",
    "    \n",
    "    # Find SNPs within 200kb of the gene's position\n",
    "    nearby_snps = significant_gwas_snps[\n",
    "        (significant_gwas_snps['base_pair_location'] >= gene_pos - window_size) &\n",
    "        (significant_gwas_snps['base_pair_location'] <= gene_pos + window_size)\n",
    "    ]\n",
    "    \n",
    "    # If we find any nearby SNPs for this gene, store the results\n",
    "    if not nearby_snps.empty:\n",
    "        for _, snp_row in nearby_snps.iterrows():\n",
    "            # Combine gene information with the full SNP row\n",
    "            result_row = gene_row.to_dict()  # Convert gene row to a dictionary\n",
    "            result_row.update(snp_row.to_dict())  # Update with SNP row\n",
    "            results.append(result_row)\n",
    "\n",
    "# Convert results to a DataFrame with all columns from `sum` and `significant_gwas_snps`\n",
    "merged_results = pd.DataFrame(results)\n",
    "merged_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 6a: for each gene from step 5, select all variants within 200kb from the TSS (upstream and downstream) from the eQTL all effect sum stats (step 3)\n",
    "\n",
    "filtered_results = merged_results.drop_duplicates(subset=[\"Gene\", \"GenePos\", \"SNP\"])\n",
    "\n",
    "# Dictionary to store output with (Gene, GenePos) as keys\n",
    "output_dict = {}\n",
    "\n",
    "# Loop over each row in filtered_results\n",
    "for _, row in filtered_results.iterrows():\n",
    "    gene = row[\"Gene\"]\n",
    "    gene_pos = row[\"GenePos\"]\n",
    "\n",
    "    # Filter final_output based on matching Gene and GenePos within 200kb range\n",
    "    matches = final_output[\n",
    "        (final_output[\"Gene\"] == gene) &\n",
    "        (final_output[\"GenePos\"].between(gene_pos - 200000, gene_pos + 200000))\n",
    "    ]\n",
    "    \n",
    "    # Store the result in the dictionary with (Gene, GenePos) as the key\n",
    "    output_dict[(gene, gene_pos)] = matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique Gene and GenePos combinations\n",
    "gene_pos_combinations = filtered_results[['Gene', 'GenePos']].drop_duplicates()\n",
    "\n",
    "# Convert to a list of dictionaries\n",
    "gene_pos_list = gene_pos_combinations.to_dict(orient='records')\n",
    "\n",
    "# Save the list as a JSON file\n",
    "with open('gene_pos_combinations.json', 'w') as f:\n",
    "    json.dump(gene_pos_list, f, indent=4)\n",
    "\n",
    "# Optionally, print the first few combinations to check\n",
    "print(gene_pos_list[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 6b: select the overlapping variants from the GWAS sum stats\n",
    "gwas_overlap_dict = {}\n",
    "\n",
    "# Loop over each (Gene, GenePos) key in output_dict\n",
    "for (gene, gene_pos), matches_df in output_dict.items():\n",
    "    \n",
    "    # Get the unique SNPs from the matches DataFrame\n",
    "    snps = matches_df[\"SNP\"].unique()\n",
    "    \n",
    "    # Filter full_gwas based on matching rsid\n",
    "    gwas_matches = gwas[gwas[\"rsid\"].isin(snps)]\n",
    "    \n",
    "    # Store the result in gwas_overlap_dict with (Gene, GenePos) as the key\n",
    "    gwas_overlap_dict[(gene, gene_pos)] = gwas_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save dictionaries to a file\n",
    "# with open('output_dict.pkl', 'wb') as f:\n",
    "#     pickle.dump(output_dict, f)\n",
    "\n",
    "# with open('gwas_overlap_dict.pkl', 'wb') as f:\n",
    "#     pickle.dump(gwas_overlap_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load in dictonaries\n",
    "# with open('output_dict.pkl', 'rb') as f:\n",
    "#     output_dict = pickle.load(f)\n",
    "\n",
    "# with open('gwas_overlap_dict.pkl', 'rb') as f:\n",
    "#     gwas_overlap_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick data inspection test\n",
    "gene = \"ENSG00000165171\"\n",
    "gene_pos = 73834590\n",
    "\n",
    "# Access the DataFrame from output_dict\n",
    "result_df = output_dict.get((gene, gene_pos), None)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas_result_df = gwas_overlap_dict.get((gene, gene_pos), None)\n",
    "gwas_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data manipulation and inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas_df = gwas.copy()\n",
    "sum_df = sum.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas_df = gwas_df[gwas_df[\"p_value\"] < 5e-8]\n",
    "gwas_df.chromosome.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas_df = gwas_df.drop_duplicates(subset='rsid', keep='first')\n",
    "gwas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_df.Gene.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_df = sum_df.drop_duplicates(subset='SNP', keep='first')\n",
    "sum_df.Gene.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas_df[\"varbeta\"] = gwas_df[\"standard_error\"] ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_df[\"varbeta\"] = sum_df[\"MetaSE\"] ** 2\n",
    "sum_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export adjusted sum stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas_df.to_csv(\"/groups/umcg-fg/tmp04/projects/gut-bulk/ongoing/2024-02-07-GutPublicRNASeq/extra_scripts/coloc/gwas.tsv\", sep=\"\\t\", header=True, index=False)\n",
    "sum_df.to_csv(\"/groups/umcg-fg/tmp04/projects/gut-bulk/ongoing/2024-02-07-GutPublicRNASeq/extra_scripts/coloc/sum.tsv\", sep=\"\\t\", header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
